# **Research Proposal: Integrating Generative AI with QLab Using Open Sound Control (OSC)**

## **1. Project Overview**
### **Title:**  
**AI-Driven Media Control for QLab Using Open Sound Control (OSC)**  

### **Student:**  
Edward Gonzalez  
Collaborating with Kazi Islam

### **Advisor:**  
Dr. David Smith  

### **Program/Department:**  
Emerging Media Technology  

### **Credits:**  
3 Academic Credits  

### **Semester:**  
Spring 2025  

### **Duration:**  
15 Weeks (~135 hours)  

---

## **2. Project Objectives**
This research project seeks to integrate **Generative AI (GAI)** with **QLab**, a multimedia control software, using **Open Sound Control (OSC)**. The goal is to develop an **intelligent, real-time media control system** that:
- Dynamically **selects and triggers media cues** (images, video, sound, lighting) based on AI decision-making.
- **Learns from user feedback**, refining selections over time.
- **Expands beyond file names**, enabling AI to **understand and classify media semantically**.
- Incorporates **live performance conditions**, including **performer input, gestures, and real-time audience feedback**.

---

## **3. Research Justification & Significance**
- **Advances AI Integration in Live Performance:**  
  Develops an AI-assisted **adaptive media control system** that enhances creativity.
- **Applies AI-Based Media Understanding:**  
  AI will **analyze, describe, and map images/videos** to **textual prompts**, enabling more intelligent cue selection.
- **Expands Open-Source Capabilities for QLab:**  
  Demonstrates how **AI-driven automation** can optimize real-time **multimedia performance control**.
- **Connects to the Blended Shadow Puppet Meta-Project:**  
  Supports **interactive storytelling** and **automated visual performance systems**.

---

## **4. System Architecture Overview**
The AI-driven media control system consists of the following **core components**:

| **Component** | **Description** |
|--------------|----------------|
| **User Input** | Prompt-based input (text, gestures, sensor data) |
| **Media Database** | Image, video, sound, and lighting assets |
| **AI Processing** | NLP + Computer Vision to generate media descriptors |
| **Matching Engine** | AI selects most relevant cue based on input & past feedback |
| **QLab Cue Triggering** | AI sends OSC messages to QLab |
| **Performance Execution** | Real-time control of visuals, sound, lighting |
| **User Feedback System** | AI adjusts based on real-time evaluation |

---

## **5. Timeline: Five-Stage Research & Development Plan**
The project follows a structured **five-stage approach** to ensure effective implementation.

| **Stage** | **Key Focus Areas** | **Weeks** |
|----------|--------------------|----------|
| **1. Research** | AI tools evaluation, dataset preparation, text-to-media mapping | **1-5** |
| **2. Design** | AI cue matching, feedback system enhancement | **6-10** |
| **3. Produce** | AI-QLab integration, prototype testing | **11-15** |
| **4. Publish** | Live performance tests, documentation | **16-18** |
| **5. Assess** | Evaluation, refinement, future scalability | **19-20** |

---

## **6. AI-Driven Image & Cue Selection: Methodology**
### **A. AI-Generated Media Descriptions**
- AI **analyzes images/videos** and assigns **semantic descriptions**.
- Example:  
  - **Image Input:** Dark forest scene.  
  - **AI Description:** `"Dark, eerie forest with mist. High contrast, cinematic lighting."`
- Enables AI to **match text prompts with visual assets.**

### **B. Text-to-Image Matching for AI Cue Selection**
- AI **receives a word or phrase prompt** (e.g., `"dreamlike"`, `"chaotic energy"`).
- AI **retrieves best-matching media** based on **semantic similarity**.
- Uses **Natural Language Processing (NLP) + Computer Vision models** (e.g., OpenAI CLIP, Google Vision).

### **C. Multi-Modal Input Processing**
AI incorporates **various input sources** for dynamic cue control:
- **Performer Gestures** → Selects visual effects based on movement.
- **Music Features** → Adjusts cue selection based on **tempo, intensity**.
- **Real-Time Audience Feedback** → AI adapts cues to engagement levels.

### **D. AI Learning & Feedback Optimization**
- AI records **cue effectiveness** and **adapts future selections**.
- AI tracks **which cues perform best** based on user preferences.

---

## **7. Implementation Steps**
### **Stage 1: Research (Weeks 1-5)**
- **Compare AI APIs for media analysis:**  
  - **Google Vision, OpenAI CLIP, RunwayML**
- **Train AI for text-to-media mapping**  
- **Build initial dataset for AI testing**  
- **Deliverables:**  
  ✅ AI-generated media descriptors  
  ✅ Initial **text-to-image matching prototype**  

### **Stage 2: Design (Weeks 6-10)**
- **Develop AI cue matching engine**  
- **Enhance AI’s image classification capabilities**  
- **User feedback system refinement**  
- **Deliverables:**  
  ✅ AI-powered **text-to-media cueing system**  
  ✅ AI-driven **image and video tagging system**  

### **Stage 3: Produce (Weeks 11-15)**
- **Integrate AI with QLab via OSC**  
- **Test AI cue selection in real-time**  
- **Solve QLab ‘GO1’ trigger issue**  
- **Deliverables:**  
  ✅ Fully integrated **AI-driven QLab cue control**  
  ✅ Initial performance tests  

### **Stage 4: Publish (Weeks 16-18)**
- **Simulated and live tests**  
- **Evaluate AI’s cue accuracy & responsiveness**  
- **Document best practices for AI-driven performance control**  
- **Deliverables:**  
  ✅ Performance test data  
  ✅ Video documentation  

### **Stage 5: Assess (Weeks 19-20)**
- **Analyze AI effectiveness in live settings**  
- **Improve AI’s adaptive learning over multiple performances**  
- **Plan for future scalability and research**  
- **Deliverables:**  
  ✅ Final **performance evaluation report**  
  ✅ Scalability roadmap  

---

## **8. Expected Outcomes**
- **Enhanced QLab AI control** with real-time media adaptation.
- **Dynamic AI cueing system** for live performance automation.
- **Scalable AI-QLab framework** for future projects in **Blended Shadow Puppet Theatre** and beyond.

---

## **9. Future Research Directions**
- **AI improvisation in live performances** (real-time media synthesis).  
- **Integration with VR & interactive storytelling**.  
- **Multi-user AI collaboration for large-scale productions**.  

---

## **10. Evaluation Criteria**
| **Metric** | **Weight (%)** | **Description** |
|-----------|---------------|----------------|
| **Program Functionality** | 40% | AI’s ability to process inputs & trigger media cues. |
| **AI Training & Feedback** | 25% | AI’s learning & refinement over time. |
| **Live Performance Testing** | 20% | AI’s effectiveness in real-world scenarios. |
| **Final Report & Documentation** | 15% | Clarity, structure, and usability of findings. |

---

## **11. Conclusion**
This research extends **beyond traditional QLab automation** to **AI-driven creative decision-making**. By implementing **text-based AI cueing, multi-modal input analysis, and real-time learning**, this project will develop a **fully autonomous, adaptive media control system** for **live performances and beyond**.

---

**Prepared by:**  
**Edward Gonzalez**  
**Spring 2025**
